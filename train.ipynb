{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import conll2002\n",
    "from nltk.corpus.reader import ConllCorpusReader\n",
    "from nltk.corpus.reader import ChunkedCorpusReader\n",
    "\n",
    "import nltk.chunk.named_entity\n",
    "from nltk.classify import MaxentClassifier\n",
    "import pickle\n",
    "\n",
    "nltk.config_megam('../../megam_i686.opt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.config_megam('../../megam_i686.opt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessingConll2003(filename, new_filename):\n",
    "    with open(filename) as f:\n",
    "        lines = f.readlines()\n",
    "    filtered_lines = [line for line in lines if (\"docstart\" not in line.lower())]\n",
    "    with open(new_filename, \"w\") as f:\n",
    "        f.writelines(filtered_lines)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessingConll2003('../../data/conll2003/deu.all', '../../data/conll2003/deu.final.all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = ConllCorpusReader('../../data/conll2003', 'deu.final.all', ['words', 'ignore', 'pos', 'ignore', 'chunk'])\n",
    "\n",
    "ne_chunker = nltk.chunk.named_entity.NEChunkParser(train.chunked_sents())\n",
    "pickle.dump(ne_chunker, open('../../models/nltkconllfinal', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = ConllCorpusReader('../conll2003ger', 'deu.train.preprocessed', ['words', 'ignore', 'pos', 'ignore', 'chunk'])\n",
    "\n",
    "ne_chunker = nltk.chunk.named_entity.NEChunkParser(train.chunked_sents())\n",
    "filename = '../models/conll2003ger'\n",
    "pickle.dump(ne_chunker, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChunkParse score:\n",
      "    IOB Accuracy:  93.4%%\n",
      "    Precision:     67.8%%\n",
      "    Recall:        49.1%%\n",
      "    F-Measure:     56.9%%\n",
      "<class 'nltk.chunk.util.ChunkScore'>\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "DataFrame constructor not properly called!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-a5bad6460004>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloaded\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_sents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    466\u001b[0m                                    dtype=values.dtype, copy=False)\n\u001b[1;32m    467\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 468\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'DataFrame constructor not properly called!'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    469\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    470\u001b[0m         \u001b[0mNDFrame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmgr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfastpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: DataFrame constructor not properly called!"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "test = ConllCorpusReader('../../conll2003ger', 'deu.testa', ['words', 'ignore', 'pos', 'ignore', 'chunk'])\n",
    "test_sents = test.chunked_sents()\n",
    "\n",
    "loaded = pickle.load(open('../../models/conll2003ger', 'rb'))\n",
    "print(loaded.evaluate(test_sents))\n",
    "\n",
    "results = loaded.evaluate(test_sents)\n",
    "print(type(results))\n",
    "df = pd.DataFrame(results)\n",
    "print(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChunkParse score:\n",
      "    IOB Accuracy:  93.4%%\n",
      "    Precision:     67.8%%\n",
      "    Recall:        49.1%%\n",
      "    F-Measure:     56.9%%\n"
     ]
    }
   ],
   "source": [
    "# print(results.precision())\n",
    "# tn = results._tags_total - results._tp_num - results._fp_num- results._fn_num\n",
    "# print(results._tags_correct)\n",
    "# results_dict = {\n",
    "#             'Metrics': ['IOB Accuracy','Precision:','Recall','F-Measure', 'tp', 'fp', 'fn', 'tn'],\n",
    "#             'Score': [results.accuracy() ,results.precision(),results.recall(),results.f_measure(),results._tp_num, results._fp_num, results._fn_num, tn ]\n",
    "#           }\n",
    "# df = pd.DataFrame(results_dict)\n",
    "# df.to_csv('demo.csv', index = False)\n",
    "# print(df)\n",
    "# print(results.missed())\n",
    "# print()\n",
    "# print()\n",
    "# print()\n",
    "# print(results.incorrect())\n",
    "# print()\n",
    "# print()\n",
    "# print()\n",
    "# print(results.correct())\n",
    "# print()\n",
    "# print()\n",
    "# print()\n",
    "# print(results.guessed())\n",
    "\n",
    "print(results)\n",
    "\n",
    "columnlist = ['IOB Accuracy','Precision:','Recall','F-Measure', 'True Positives', 'False Positives', 'False Negatives']\n",
    "df = pd.DataFrame(columns= columnlist)\n",
    "\n",
    "bufferdf = pd.DataFrame([[results.accuracy() ,results.precision(),results.recall(),results.f_measure(),results._tp_num, results._fp_num, results._fn_num]], columns = columnlist, index = ['Fold' + str(1)])\n",
    "\n",
    "df = df.append(bufferdf, verify_integrity=True)\n",
    "\n",
    "df.to_csv('demo1.csv')\n",
    "bufferdf.to_csv('demo2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChunkParse score:\n",
      "    IOB Accuracy: 100.0%%\n",
      "    Precision:      0.0%%\n",
      "    Recall:         0.0%%\n",
      "    F-Measure:      0.0%%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train = ConllCorpusReader('../../data/GermEval2014/split', 'deu.1', ['ignore', 'words', 'chunk', 'chunk', 'pos'])\n",
    "\n",
    "ne_chunker = nltk.chunk.named_entity.NEChunkParser(train.chunked_sents())\n",
    "\n",
    "test = ConllCorpusReader('../../data/GermEval2014/split', 'deu.2', ['ignore', 'words', 'chunk', 'chunk', 'pos'])\n",
    "test_sents = test.chunked_sents()\n",
    "\n",
    "\n",
    "\n",
    "results = ne_chunker.evaluate(test_sents)\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChunkParse score:\n",
      "    IOB Accuracy:  99.4%%\n",
      "    Precision:     66.7%%\n",
      "    Recall:         5.0%%\n",
      "    F-Measure:      9.4%%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train = ConllCorpusReader('../../data/GermEval2014', 'NER-de-dev.nohashtag.new.tsv', ['ignore', 'words', 'chunk', 'chunk', 'pos'])\n",
    "\n",
    "ne_chunker = nltk.chunk.named_entity.NEChunkParser(train.chunked_sents())\n",
    "\n",
    "test = ConllCorpusReader('../../data/GermEval2014', 'NER-de-test.pos.nohashtag.tsv', ['ignore', 'words', 'chunk', 'chunk', 'pos'])\n",
    "test_sents = test.chunked_sents()\n",
    "\n",
    "\n",
    "\n",
    "results = ne_chunker.evaluate(test_sents)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChunkParse score:\n",
      "    IOB Accuracy:  92.2%%\n",
      "    Precision:     65.2%%\n",
      "    Recall:        23.2%%\n",
      "    F-Measure:     34.3%%\n"
     ]
    }
   ],
   "source": [
    "train = ConllCorpusReader('../../data/GermEval2014', 'NER-de-dev.nohashtag.new.tsv', ['ignore', 'words', 'chunk', 'ignore', 'pos'])\n",
    "\n",
    "ne_chunker = nltk.chunk.named_entity.NEChunkParser(train.chunked_sents())\n",
    "\n",
    "test = ConllCorpusReader('../../data/GermEval2014', 'NER-de-test.pos.nohashtag.tsv', ['ignore', 'words', 'chunk', 'ignore', 'pos'])\n",
    "test_sents = test.chunked_sents()\n",
    "\n",
    "\n",
    "\n",
    "results = ne_chunker.evaluate(test_sents)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChunkParse score:\n",
      "    IOB Accuracy:  99.4%%\n",
      "    Precision:     66.7%%\n",
      "    Recall:         5.0%%\n",
      "    F-Measure:      9.4%%\n"
     ]
    }
   ],
   "source": [
    "train = ConllCorpusReader('../../data/GermEval2014', 'NER-de-dev.nohashtag.new.tsv', ['ignore', 'words', 'ignore', 'chunk', 'pos'])\n",
    "\n",
    "ne_chunker = nltk.chunk.named_entity.NEChunkParser(train.chunked_sents())\n",
    "\n",
    "test = ConllCorpusReader('../../data/GermEval2014', 'NER-de-test.pos.nohashtag.tsv', ['ignore', 'words', 'ignore', 'chunk', 'pos'])\n",
    "test_sents = test.chunked_sents()\n",
    "\n",
    "\n",
    "\n",
    "results = ne_chunker.evaluate(test_sents)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChunkParse score:\n",
      "    IOB Accuracy:  85.4%%\n",
      "    Precision:     39.2%%\n",
      "    Recall:        36.6%%\n",
      "    F-Measure:     37.9%%\n"
     ]
    }
   ],
   "source": [
    "train = ConllCorpusReader('../../data/LegalER/dataset_courts', 'bag.new.conll', ['words', 'chunk', 'pos'])\n",
    "\n",
    "ne_chunker = nltk.chunk.named_entity.NEChunkParser(train.chunked_sents())\n",
    "\n",
    "test = ConllCorpusReader('../../data/LegalER/dataset_courts', 'bfh.new.conll', ['words', 'chunk', 'pos'])\n",
    "test_sents = test.chunked_sents()\n",
    "\n",
    "results = ne_chunker.evaluate(test_sents)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChunkParse score:\n",
      "    IOB Accuracy:  86.1%%\n",
      "    Precision:     42.4%%\n",
      "    Recall:        42.1%%\n",
      "    F-Measure:     42.3%%\n"
     ]
    }
   ],
   "source": [
    "train = ConllCorpusReader('../../data/LegalER/dataset_courts/coarse-grained', 'bag.new.conll', ['words', 'chunk', 'pos'])\n",
    "\n",
    "ne_chunker = nltk.chunk.named_entity.NEChunkParser(train.chunked_sents())\n",
    "\n",
    "test = ConllCorpusReader('../../data/LegalER/dataset_courts/coarse-grained', 'bfh.new.conll', ['words', 'chunk', 'pos'])\n",
    "test_sents = test.chunked_sents()\n",
    "\n",
    "results = ne_chunker.evaluate(test_sents)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "#WNUT 17\n",
    "\n",
    "train = ConllCorpusReader('../../data/WNUT17/CONLL-format/data/test', 'eng.1', ['words', 'chunk', 'pos'])\n",
    "\n",
    "train_sents = train.chunked_sents()\n",
    "\n",
    "ne_chunker = nltk.chunk.named_entity.NEChunkParser(train_sents)\n",
    "\n",
    "# test = ConllCorpusReader('../../data/WNUT17/CONLL-format/data/test', 'emerging.test.pos.annotated', ['words', 'chunk', 'pos'])\n",
    "# test_sents = test.chunked_sents()\n",
    "\n",
    "# results = ne_chunker.evaluate(test_sents)\n",
    "# print(results)\n",
    "# maybe some characters interfering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = ConllCorpusReader('../../data/WNUT17/CONLL-format/data/train', 'wnut17train.pos.pp.conll', ['words', 'chunk', 'pos'])\n",
    "\n",
    "train_sents = train.chunked_sents()\n",
    "\n",
    "ne_chunker = nltk.chunk.named_entity.NEChunkParser(train_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChunkParse score:\n",
      "    IOB Accuracy:  92.6%%\n",
      "    Precision:     37.3%%\n",
      "    Recall:         3.5%%\n",
      "    F-Measure:      6.4%%\n"
     ]
    }
   ],
   "source": [
    "test = ConllCorpusReader('../../data/WNUT17/CONLL-format/data/test', 'emerging.test.pos.pp.annotated', ['words', 'chunk', 'pos'])\n",
    "test_sents = test.chunked_sents()\n",
    "\n",
    "results = ne_chunker.evaluate(test_sents)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "concat() expects at least one object!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-117-4bb7a2a51b68>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mConllCorpusReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../../data/WNUT17/CONLL-format/data/train'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wnut17.train.pos.pp.conll'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'words'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'chunk'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'pos'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtrain_sents\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunked_sents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mne_chunker\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnamed_entity\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNEChunkParser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_sents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/nltk/corpus/reader/conll.py\u001b[0m in \u001b[0;36mchunked_sents\u001b[0;34m(self, fileids, chunk_types, tagset)\u001b[0m\n\u001b[1;32m    152\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_chunked_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunk_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtagset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mLazyMap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_chunked_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_grids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfileids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mparsed_sents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfileids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos_in_tree\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtagset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/nltk/corpus/reader/conll.py\u001b[0m in \u001b[0;36m_grids\u001b[0;34m(self, fileids)\u001b[0m\n\u001b[1;32m    220\u001b[0m             [\n\u001b[1;32m    221\u001b[0m                 \u001b[0mStreamBackedCorpusView\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfileid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_grid_block\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0menc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 222\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfileid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabspaths\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfileids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    223\u001b[0m             ]\n\u001b[1;32m    224\u001b[0m         )\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/nltk/corpus/reader/util.py\u001b[0m in \u001b[0;36mconcat\u001b[0;34m(docs)\u001b[0m\n\u001b[1;32m    439\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdocs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 441\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"concat() expects at least one object!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    442\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m     \u001b[0mtypes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdocs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: concat() expects at least one object!"
     ]
    }
   ],
   "source": [
    "train = ConllCorpusReader('../../data/WNUT17/CONLL-format/data/train', 'wnut17.train.pos.pp.conll', ['words', 'chunk', 'pos'])\n",
    "\n",
    "train_sents = train.chunked_sents()\n",
    "\n",
    "ne_chunker = nltk.chunk.named_entity.NEChunkParser(train_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 2, got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-109-105f074949c9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtrain_sents\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunked_sents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mne_chunker\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnamed_entity\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNEChunkParser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_sents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mConllCorpusReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../../data/EuropeanaNewspapers/enp_DE.onb.bio'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'enp_DE.onb.pos.bio'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'words'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'chunk'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'pos'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/nltk/chunk/named_entity.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, train)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/nltk/chunk/named_entity.py\u001b[0m in \u001b[0;36m_train\u001b[0;34m(self, corpus)\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m         \u001b[0;31m# Convert to tagged sequence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m         \u001b[0mcorpus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parse_to_tagged\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tagger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNEChunkParserTagger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/nltk/chunk/named_entity.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m         \u001b[0;31m# Convert to tagged sequence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m         \u001b[0mcorpus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parse_to_tagged\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tagger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNEChunkParserTagger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/nltk/collections.py\u001b[0m in \u001b[0;36miterate_from\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    405\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lists\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_all_lazy\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lists\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterate_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 407\u001b[0;31m                 \u001b[0;32myield\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    408\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/nltk/corpus/reader/conll.py\u001b[0m in \u001b[0;36mget_chunked_words\u001b[0;34m(grid)\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mget_chunked_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# capture chunk_types as local var\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_chunked_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunk_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtagset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mLazyMap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_chunked_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_grids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfileids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/nltk/corpus/reader/conll.py\u001b[0m in \u001b[0;36m_get_chunked_words\u001b[0;34m(self, grid, chunk_types, tagset)\u001b[0m\n\u001b[1;32m    286\u001b[0m                 \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunk_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"O\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 288\u001b[0;31m                 \u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunk_type\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchunk_tag\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"-\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    289\u001b[0m             \u001b[0;31m# If it's a chunk we don't care about, treat it as O.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mchunk_types\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mchunk_type\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mchunk_types\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 1)"
     ]
    }
   ],
   "source": [
    "train = ConllCorpusReader('../../data/EuropeanaNewspapers/enp_DE.lft.bio', 'enp_DE.lft.pos.bio', ['words', 'chunk', 'pos'])\n",
    "\n",
    "train_sents = train.chunked_sents()\n",
    "\n",
    "ne_chunker = nltk.chunk.named_entity.NEChunkParser(train_sents)\n",
    "\n",
    "test = ConllCorpusReader('../../data/EuropeanaNewspapers/enp_DE.onb.bio', 'enp_DE.onb.pos.bio', ['words', 'chunk', 'pos'])\n",
    "test_sents = test.chunked_sents()\n",
    "\n",
    "results = ne_chunker.evaluate(test_sents)\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mInit signature:\u001b[0m\n",
       "\u001b[0mConllCorpusReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mfileids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mcolumntypes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mchunk_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mroot_label\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'S'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mpos_in_tree\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0msrl_includes_roleset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf8'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mtree_class\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m<\u001b[0m\u001b[0;32mclass\u001b[0m \u001b[0;34m'nltk.tree.Tree'\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mtagset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mseparator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m     \n",
       "A corpus reader for CoNLL-style files.  These files consist of a\n",
       "series of sentences, separated by blank lines.  Each sentence is\n",
       "encoded using a table (or \"grid\") of values, where each line\n",
       "corresponds to a single word, and each column corresponds to an\n",
       "annotation type.  The set of columns used by CoNLL-style files can\n",
       "vary from corpus to corpus; the ``ConllCorpusReader`` constructor\n",
       "therefore takes an argument, ``columntypes``, which is used to\n",
       "specify the columns that are used by a given corpus. By default\n",
       "columns are split by consecutive whitespaces, with the\n",
       "``separator`` argument you can set a string to split by (e.g.\n",
       "``' '``).\n",
       "\n",
       "\n",
       "@todo: Add support for reading from corpora where different\n",
       "    parallel files contain different columns.\n",
       "@todo: Possibly add caching of the grid corpus view?  This would\n",
       "    allow the same grid view to be used by different data access\n",
       "    methods (eg words() and parsed_sents() could both share the\n",
       "    same grid corpus view object).\n",
       "@todo: Better support for -DOCSTART-.  Currently, we just ignore\n",
       "    it, but it could be used to define methods that retrieve a\n",
       "    document at a time (eg parsed_documents()).\n",
       "\u001b[0;31mInit docstring:\u001b[0m\n",
       ":type root: PathPointer or str\n",
       ":param root: A path pointer identifying the root directory for\n",
       "    this corpus.  If a string is specified, then it will be\n",
       "    converted to a ``PathPointer`` automatically.\n",
       ":param fileids: A list of the files that make up this corpus.\n",
       "    This list can either be specified explicitly, as a list of\n",
       "    strings; or implicitly, as a regular expression over file\n",
       "    paths.  The absolute path for each file will be constructed\n",
       "    by joining the reader's root to each file name.\n",
       ":param encoding: The default unicode encoding for the files\n",
       "    that make up the corpus.  The value of ``encoding`` can be any\n",
       "    of the following:\n",
       "    - A string: ``encoding`` is the encoding name for all files.\n",
       "    - A dictionary: ``encoding[file_id]`` is the encoding\n",
       "      name for the file whose identifier is ``file_id``.  If\n",
       "      ``file_id`` is not in ``encoding``, then the file\n",
       "      contents will be processed using non-unicode byte strings.\n",
       "    - A list: ``encoding`` should be a list of ``(regexp, encoding)``\n",
       "      tuples.  The encoding for a file whose identifier is ``file_id``\n",
       "      will be the ``encoding`` value for the first tuple whose\n",
       "      ``regexp`` matches the ``file_id``.  If no tuple's ``regexp``\n",
       "      matches the ``file_id``, the file contents will be processed\n",
       "      using non-unicode byte strings.\n",
       "    - None: the file contents of all files will be\n",
       "      processed using non-unicode byte strings.\n",
       ":param tagset: The name of the tagset used by this corpus, to be used\n",
       "      for normalizing or converting the POS tags returned by the\n",
       "      tagged_...() methods.\n",
       "\u001b[0;31mFile:\u001b[0m           ~/.local/lib/python3.5/site-packages/nltk/corpus/reader/conll.py\n",
       "\u001b[0;31mType:\u001b[0m           type\n",
       "\u001b[0;31mSubclasses:\u001b[0m     ConllChunkCorpusReader\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ConllCorpusReader?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(nltk.corpus.reader.conll.ConllCorpusReader,\n",
       " nltk.corpus.reader.api.CorpusReader,\n",
       " object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.__class__.__mro__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CHUNK',\n",
       " 'COLUMN_TYPES',\n",
       " 'IGNORE',\n",
       " 'NE',\n",
       " 'POS',\n",
       " 'SRL',\n",
       " 'TREE',\n",
       " 'WORDS',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_chunk_types',\n",
       " '_colmap',\n",
       " '_encoding',\n",
       " '_fileids',\n",
       " '_get_chunked_words',\n",
       " '_get_column',\n",
       " '_get_iob_words',\n",
       " '_get_parsed_sent',\n",
       " '_get_root',\n",
       " '_get_srl_instances',\n",
       " '_get_srl_spans',\n",
       " '_get_tagged_words',\n",
       " '_get_words',\n",
       " '_grids',\n",
       " '_pos_in_tree',\n",
       " '_read_grid_block',\n",
       " '_require',\n",
       " '_root',\n",
       " '_root_label',\n",
       " '_srl_includes_roleset',\n",
       " '_tagset',\n",
       " '_tree_class',\n",
       " 'abspath',\n",
       " 'abspaths',\n",
       " 'chunked_sents',\n",
       " 'chunked_words',\n",
       " 'citation',\n",
       " 'encoding',\n",
       " 'ensure_loaded',\n",
       " 'fileids',\n",
       " 'iob_sents',\n",
       " 'iob_words',\n",
       " 'license',\n",
       " 'open',\n",
       " 'parsed_sents',\n",
       " 'raw',\n",
       " 'readme',\n",
       " 'root',\n",
       " 'sents',\n",
       " 'sep',\n",
       " 'srl_instances',\n",
       " 'srl_spans',\n",
       " 'tagged_sents',\n",
       " 'tagged_words',\n",
       " 'words']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Return an open stream that can be used to read the given file.\n",
       "If the file's encoding is not None, then the stream will\n",
       "automatically decode the file's contents into unicode.\n",
       "\n",
       ":param file: The file identifier of the file to read.\n",
       "\u001b[0;31mFile:\u001b[0m      ~/.local/lib/python3.5/site-packages/nltk/corpus/reader/api.py\n",
       "\u001b[0;31mType:\u001b[0m      method\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train.open?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 78.4 Âµs\n"
     ]
    }
   ],
   "source": [
    "a = 1\n",
    "b = 2\n",
    "%time print(a+b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
