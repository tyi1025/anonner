{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "128085f7-2cbe-4d38-8918-0d45724cdddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.tokenizer import Tokenizer\n",
    "from spacy.lang.en import English\n",
    "import pycrfsuite\n",
    "from spacy.tokenizer import Tokenizer\n",
    "from spacy.lang.en import English\n",
    "import random\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score, confusion_matrix, plot_confusion_matrix, ConfusionMatrixDisplay\n",
    "import os\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09078cb0-ec45-4820-8ac0-caa19b8bb370",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b264ca5e-dd61-48c8-9fea-a2b6c468b9b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model's parameters : ['feature.minfreq', 'feature.possible_states', 'feature.possible_transitions', 'c1', 'c2', 'max_iterations', 'num_memories', 'epsilon', 'period', 'delta', 'linesearch', 'max_linesearch']\n",
      "Last iteration log {'num': 60, 'scores': {}, 'loss': 532.023099, 'feature_norm': 40.440112, 'error_norm': 93.032465, 'active_features': 1575, 'linesearch_trials': 1, 'linesearch_step': 1.0, 'time': 0.037}\n",
      "sentence: NO PLEDGE OR LOAN ; CHARACTERIZATION . \n",
      "predicted labels: ['O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "real labels ['O', 'O', 'O', 'O', 'O', 'O', 'O']\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "You appear to be using a legacy multi-label data representation. Sequence of sequences are no longer supported; use a binary array or sparse matrix instead - the MultiLabelBinarizer transformer can convert to this format.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1091115/3526149842.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0msec_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_text_labeled_sentences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msec_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msec_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_1091115/1215548725.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;31m#     print(\"conf_matrix: \\n\", confusion_matrix(targets, outputs))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"accuracy score:\\n\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"precision score:\\n\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprecision_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'micro'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"recall score:\\n\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecall_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'micro'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;31m# extra_args > 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36maccuracy_score\u001b[0;34m(y_true, y_pred, normalize, sample_weight)\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m     \u001b[0;31m# Compute accuracy for each possible representation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 202\u001b[0;31m     \u001b[0my_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    203\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'multilabel'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     82\u001b[0m     \"\"\"\n\u001b[1;32m     83\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m     \u001b[0mtype_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m     \u001b[0mtype_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/utils/multiclass.py\u001b[0m in \u001b[0;36mtype_of_target\u001b[0;34m(y)\u001b[0m\n\u001b[1;32m    277\u001b[0m         if (not hasattr(y[0], '__array__') and isinstance(y[0], Sequence)\n\u001b[1;32m    278\u001b[0m                 and not isinstance(y[0], str)):\n\u001b[0;32m--> 279\u001b[0;31m             raise ValueError('You appear to be using a legacy multi-label data'\n\u001b[0m\u001b[1;32m    280\u001b[0m                              \u001b[0;34m' representation. Sequence of sequences are no'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m                              \u001b[0;34m' longer supported; use a binary array or sparse'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: You appear to be using a legacy multi-label data representation. Sequence of sequences are no longer supported; use a binary array or sparse matrix instead - the MultiLabelBinarizer transformer can convert to this format."
     ]
    }
   ],
   "source": [
    "nlp = English()\n",
    "    # Create a blank Tokenizer with just the English vocab\n",
    "tokenizer = Tokenizer(nlp.vocab)\n",
    "# filepath = '../../data/conll2003/eng.all'\n",
    "filepath = '../../data/SEC-filings/CONLL-format/data/eng.all'\n",
    "sec_data = read_text_labeled_sentences(filepath, tokenizer)\n",
    "random.shuffle(sec_data)\n",
    "main(sec_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7094acc3-7fd2-416b-94ed-5af099528bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_text_labeled_sentences(filepath, tokenizer):\n",
    "    with open(filepath) as f:\n",
    "        lines = f.readlines() # list containing lines of file\n",
    "    sentence = \"\"\n",
    "    labels = []\n",
    "    TRAINING_DATA = []\n",
    "    for line in lines:\n",
    "        if line.startswith('-DOCSTART-'): continue\n",
    "        elif line == '\\n':\n",
    "            if labels == [] and sentence == \"\": continue\n",
    "            tokens = tokenizer(sentence.lstrip())\n",
    "            TRAINING_DATA.append((tokens,labels))\n",
    "            sentence = \"\"\n",
    "            labels = []\n",
    "            \n",
    "        else: \n",
    "            elements = line.split(' ')\n",
    "#             elements = line.split('\\t')\n",
    "            word = elements[0]\n",
    "            label = elements[3][:-1]\n",
    "#             label = elements[1]\n",
    "            labels.append(label)\n",
    "            sentence += word + ' '\n",
    "    return TRAINING_DATA\n",
    "\n",
    "def main(data):\n",
    "    # Divide disease dataset into train and test sets\n",
    "    training_data = data[int(0.3*len(data)):]\n",
    "    test_data = data[:int(0.3*len(data))]\n",
    "    # Calculate features for both training and test datasets\n",
    "    X_train = [sent2features(s[0]) for s in training_data]\n",
    "    y_train = [s[1] for s in training_data]\n",
    "    X_test = [sent2features(s[0]) for s in test_data]\n",
    "    y_test = [s[1] for s in test_data]\n",
    "    # Train the model\n",
    "    train(X_train, y_train)\n",
    "\n",
    "    # Predict labels for a given sentence example\n",
    "    tagger = pycrfsuite.Tagger()\n",
    "    tagger.open(\"../CRF_NER/models/anonmodel.crfsuite\")\n",
    "    i=0\n",
    "    print(\"sentence: {}\".format(test_data[i][0]))\n",
    "    print(\"predicted labels: {}\". format(tagger.tag(X_test[i])))\n",
    "    print(\"real labels {}\".format(y_test[i]))\n",
    "    \n",
    "    # Calculate test metrics\n",
    "    outputs = []\n",
    "    for i in range(len(X_test)):\n",
    "        outputs.append(tagger.tag(X_test[i]))\n",
    "\n",
    "    targets = sum(y_test, [])\n",
    "    outputs = sum(outputs, [])\n",
    "    \n",
    "#     print(\"conf_matrix: \\n\", confusion_matrix(targets, outputs))\n",
    "    print(\"accuracy score:\\n\", accuracy_score(targets, outputs))\n",
    "    print(\"precision score:\\n\", precision_score(targets, outputs, average='micro'))\n",
    "    print(\"recall score:\\n\", recall_score(targets, outputs, average='micro'))\n",
    "    print(\"F1 score:\\n\", f1_score(targets, outputs, average='micro'))\n",
    "\n",
    "def word2features(train_sample, i):\n",
    "    token = train_sample[i]\n",
    "    word = token.text\n",
    "    features = [\n",
    "        'bias',\n",
    "        'word.lower=' + word.lower(),\n",
    "        'word[-3:]=' + word[-3:],\n",
    "        'word[-2:]=' + word[-2:],\n",
    "        'word.isupper=%s' % word.isupper(),\n",
    "        'word.istitle=%s' % word.istitle(),\n",
    "        'word.isdigit=%s' % word.isdigit(),\n",
    "        'word.pos='+token.pos_,\n",
    "        'word.dep='+token.dep_,\n",
    "        'word.is_stop=%s' %token.is_stop,\n",
    "        'word.lemma=' + token.lemma_,\n",
    "        'word.tag=' + token.tag_,\n",
    "        'word.shape=' + token.shape_,\n",
    "        'word.is_alpha=%s' %token.is_alpha,        \n",
    "    ]\n",
    "    if i > 0:\n",
    "        token1 = train_sample[i-1]\n",
    "        word1 = token1.text\n",
    "        features.extend([\n",
    "            '-1:word.lower=' + word1.lower(),\n",
    "            '-1:word.istitle=%s' % word1.istitle(),\n",
    "            '-1:word.isupper=%s' % word1.isupper(),\n",
    "            '-1:word.pos='+token1.pos_,\n",
    "            '-1:word.dep='+token1.dep_,\n",
    "            '-1:word.is_stop=%s' %token1.is_stop,\n",
    "            '-1:word.lemma=' + token1.lemma_,\n",
    "            '-1:word.tag=' + token1.tag_,\n",
    "            '-1:word.shape=' + token1.shape_,\n",
    "            '-1:word.is_alpha=%s' %token1.is_alpha,    \n",
    "        ])\n",
    "    else:\n",
    "        features.append('BOS')\n",
    "        \n",
    "    if i < len(train_sample)-1:\n",
    "        token1 = train_sample[i+1]\n",
    "        word1 = token1.text\n",
    "        features.extend([\n",
    "            '+1:word.lower=' + word1.lower(),\n",
    "            '+1:word.istitle=%s' % word1.istitle(),\n",
    "            '+1:word.isupper=%s' % word1.isupper(),\n",
    "            '+1:word.pos='+token1.pos_,\n",
    "            '+1:word.dep='+token1.dep_,\n",
    "            '+1:word.is_stop=%s' %token1.is_stop,\n",
    "            '+1:word.lemma=' + token1.lemma_,\n",
    "            '+1:word.tag=' + token1.tag_,\n",
    "            '+1:word.shape=' + token1.shape_,\n",
    "            '+1:word.is_alpha=%s' %token1.is_alpha,   \n",
    "        ])\n",
    "    else:\n",
    "        features.append('EOS')       \n",
    "    return features\n",
    "\n",
    "def sent2features(train_sample):\n",
    "    return [word2features(train_sample, i) for i in range(len(train_sample))]\n",
    "\n",
    "def train(X_train, y_train, model_path=\"../CRF_NER/models/anonmodel.crfsuite\"):\n",
    "    trainer = pycrfsuite.Trainer(verbose=False)\n",
    "\n",
    "    for xseq, yseq in zip(X_train, y_train):\n",
    "        trainer.append(xseq, yseq)\n",
    "\n",
    "    trainer.set_params({\n",
    "        'c1': 0.44,   # coefficient for L1 penalty\n",
    "        'c2': 1e-4,  # coefficient for L2 penalty\n",
    "        'max_iterations': 60,  # stop earlier\n",
    "\n",
    "        # include transitions that are possible, but not observed\n",
    "        'feature.possible_transitions': True\n",
    "    })\n",
    "    print(\"model's parameters : {}\".format(trainer.params()))\n",
    "    trainer.train(model_path)\n",
    "    print(\"Last iteration log {}\".format(trainer.logparser.last_iteration))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe176142-5d3f-44c8-a09e-21519f068545",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
